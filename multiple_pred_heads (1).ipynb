{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57ab2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# https://shorturl.at/nyzL4\n",
    "# https://shorturl.at/FO479\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598f76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\") #Setting up an existing open AI gym environment\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Represents a single transition in the environment. Used for experience replay. \"\"\"\n",
    "Experience = namedtuple('Experience',                                       \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory:\n",
    "    \"\"\"Fixed-size memory buffer to store Experience tuples.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Selects a random batch of transitions for training\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66147da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network model\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct DQN model\n",
    "        \n",
    "        @param state_size(int) : Dimensions of each state\n",
    "        @param action_size(int): Dimensions of each action\n",
    "        @param seed            : Random seed\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size,64)\n",
    "        self.fc2 = nn.Linear(64, 48)\n",
    "        self.fc3 = nn.Linear(48, 48)\n",
    "        self.fc4_1 = nn.Linear(48, action_size)\n",
    "        self.fc4_2 = nn.Linear(48, action_size)\n",
    "        self.fc4_3 = nn.Linear(48, action_size)\n",
    "        self.fc4_4 = nn.Linear(48, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward propagation of the network\n",
    "        \n",
    "        @param state: state\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred1 = self.fc4_1(x) # shape: (20,)\n",
    "        pred2 = self.fc4_2(x)\n",
    "        pred3 = self.fc4_3(x)\n",
    "        pred4 = self.fc4_4(x)\n",
    "        return pred1, pred2, pred3, pred4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc20008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters (Maybe receive these as arguments? (parse_args))\"\"\"\n",
    "MEMORY_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR = 5e-4\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.05\n",
    "EPS_DECAY = 1e-3\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent that interacts and learns from the environment.\n",
    "    Receives rewards from the environment, update DQN parameters, \"memorize\" experiences.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, disc_class, seed):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct an Agent object\n",
    "        \n",
    "        @param state_size(int) : Dimensions of each state\n",
    "        @param action_size(int): Dimensions of each action\n",
    "        @param seed            : Random seed\n",
    "        \"\"\"\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.disc_class = disc_class\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Initalize main and target DQN \n",
    "        self.main_model = DQN(state_space.shape[0], disc_class, seed).to(device)\n",
    "        self.target_model = DQN(state_space.shape[0], disc_class, seed).to(device)\n",
    "        self.target_model.load_state_dict(self.main_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.main_model.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY step)\n",
    "        self.t_step = 0\n",
    "        self.eps_step = 0\n",
    "        self.spacing = 0.1\n",
    "        \n",
    "    def step(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct an Agent object\n",
    "        \n",
    "        @param state     : Current state\n",
    "        @param action    : action taken from the current state\n",
    "        @param reward    : Reward given from the environment\n",
    "        @param next_state: Next state according to the (state,action) pair\n",
    "        \"\"\"\n",
    "        # save experience in replay memory\n",
    "        self.memory.push(state, action, reward, next_state)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step+1)%UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            \n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample(BATCH_SIZE)\n",
    "                self.learn(experiences, GAMMA)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Return action for given state following the current policy\n",
    "        \n",
    "        @param state: Current state\n",
    "        @param epsilon(float)  : epsilon for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        eps_threshold = EPSILON_MIN + (EPSILON - EPSILON_MIN) * math.exp(-1 * self.eps_step / EPS_DECAY)\n",
    "        self.eps_step += 1\n",
    "#         epsilon-greedy action selection\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                pred1, pred2, pred3, pred4 = self.main_model(state)\n",
    "                pred1 = np.argmax(pred1)\n",
    "                pred2 = np.argmax(pred2)\n",
    "                pred3 = np.argmax(pred3)\n",
    "                pred4 = np.argmax(pred4)\n",
    "                action = torch.tensor((pred1, pred2, pred3, pred4))\n",
    "#                 print(\"max:\",action, action.shape)\n",
    "                return action\n",
    "        else:\n",
    "            action = torch.randint(0, 19, (4,))\n",
    "#             print(\"random:\", action, action.shape)\n",
    "            return action\n",
    "        \n",
    "    def disct2cont(self,action):\n",
    "        return action * self.spacing - self.action_space.high\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"\n",
    "        Update parameters using the given batch of expreience tuples\n",
    "        \n",
    "        @param experiences(Tuple): tuple of (s,a,s',r) tuples\n",
    "        @param gamma(float): discount factor\n",
    "        \"\"\"\n",
    "        batch = Experience(*zip(*experiences)) # This converts batch-array of Experiences\n",
    "                                               # to Experience of batch-arrays.\n",
    "        #batch.action --> (batch_size, tensor(4,))               ==> (64, tensor(4,))\n",
    "        #batch.state --> (batch_size, tensor(1,state_size))      ==> (64, tensor(1,24))\n",
    "        #batch.next_state --> (batch_size, tensor(1,state_size)) ==> (64, tensor(1,24))\n",
    "        #batch.reward --> (batch_size, tensor(1))                ==> (64, tensor(1))\n",
    "        action_batch = torch.cat(batch.action)         # tensor(256, )\n",
    "        state_batch = torch.cat(batch.state)           # tensor(64, state_size)\n",
    "        next_state_batch = torch.cat(batch.next_state) # tensor(64, state_size)\n",
    "        reward_batch = torch.cat(batch.reward) # shape (64, )\n",
    "        print(batch.action)\n",
    "        print(action_batch)\n",
    "        self.main_model.train() # Set local network as training mode\n",
    "        self.target_model.eval() # Set target network as evaluation mode\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t)\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        q_next = self.target_model(next_state_batch) # shape: tensor (4,64,20)\n",
    "        print(q_next.shape)\n",
    "        q_eval = torch.zeros([4,64,20])\n",
    "        q_target = torch.zeros([4,64,20])\n",
    "        for heads in range(len(q_eval)):\n",
    "            q_eval[heads]  = self.main_model(state_batch)[heads] # shape: (4,64,20)\n",
    "            for batch_index in range(BATCH_SIZE):\n",
    "                q_target[heads][batch_index] = reward_batch[batch_index] + gamma * q_next[heads][batch_index].max()\n",
    "#         q_target = (q_next * gamma) + reward_batch # DQN update rule\n",
    "        print(\"q_eval:\",q_eval)\n",
    "        print(\"q_target:\",q_target)\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \"\"\" UPDATE TARGET NETWORK Don't update every step (compare) \"\"\" \n",
    "        self.soft_update(self.main_model, self.target_model, TAU)\n",
    "    \n",
    "    def soft_update(self, main_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \n",
    "        @param local_model(nn model): weights will be copied from\n",
    "        @param target_model(nn model): weights will be copied to\n",
    "        @param tau(float): interpolation parameter\n",
    "        \"\"\"\n",
    "        target_model_state_dict = target_model.state_dict()\n",
    "        main_model_state_dict = main_model.state_dict()\n",
    "        for key in main_model_state_dict: # what is the key?\n",
    "            target_model_state_dict[key] = main_model_state_dict[key]*tau + target_model_state_dict[key]*(1-tau)\n",
    "        target_model.load_state_dict(target_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(returns, window=10):\n",
    "    '''\n",
    "    Returns (iterable): list of returns over time\n",
    "    window: window for rolling mean to smooth plotted curve\n",
    "    '''\n",
    "    sns.lineplot(\n",
    "        data=pd.DataFrame(returns).rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfcfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "t 0\tAverage Score nan\n",
      "t 1\tAverage Score nan\n",
      "t 2\tAverage Score nan\n",
      "t 3\tAverage Score nan\n",
      "t 4\tAverage Score nan\n",
      "t 5\tAverage Score nan\n",
      "t 6\tAverage Score nan\n",
      "t 7\tAverage Score nan\n",
      "t 8\tAverage Score nan\n",
      "t 9\tAverage Score nan\n",
      "t 10\tAverage Score nan\n",
      "t 11\tAverage Score nan\n",
      "t 12\tAverage Score nan\n",
      "t 13\tAverage Score nan\n",
      "t 14\tAverage Score nan\n",
      "t 15\tAverage Score nan\n",
      "t 16\tAverage Score nan\n",
      "t 17\tAverage Score nan\n",
      "t 18\tAverage Score nan\n",
      "t 19\tAverage Score nan\n",
      "t 20\tAverage Score nan\n",
      "t 21\tAverage Score nan\n",
      "t 22\tAverage Score nan\n",
      "t 23\tAverage Score nan\n",
      "t 24\tAverage Score nan\n",
      "t 25\tAverage Score nan\n",
      "t 26\tAverage Score nan\n",
      "t 27\tAverage Score nan\n",
      "t 28\tAverage Score nan\n",
      "t 29\tAverage Score nan\n",
      "t 30\tAverage Score nan\n",
      "t 31\tAverage Score nan\n",
      "t 32\tAverage Score nan\n",
      "t 33\tAverage Score nan\n",
      "t 34\tAverage Score nan\n",
      "t 35\tAverage Score nan\n",
      "t 36\tAverage Score nan\n",
      "t 37\tAverage Score nan\n",
      "t 38\tAverage Score nan\n",
      "t 39\tAverage Score nan\n",
      "t 40\tAverage Score nan\n",
      "t 41\tAverage Score nan\n",
      "t 42\tAverage Score nan\n",
      "t 43\tAverage Score nan\n",
      "t 44\tAverage Score nan\n",
      "t 45\tAverage Score nan\n",
      "t 46\tAverage Score nan\n",
      "t 47\tAverage Score nan\n",
      "t 48\tAverage Score nan\n",
      "t 49\tAverage Score nan\n",
      "t 50\tAverage Score nan\n",
      "t 51\tAverage Score nan\n",
      "t 52\tAverage Score nan\n",
      "t 53\tAverage Score nan\n",
      "t 54\tAverage Score nan\n",
      "t 55\tAverage Score nan\n",
      "t 56\tAverage Score nan\n",
      "t 57\tAverage Score nan\n",
      "t 58\tAverage Score nan\n",
      "t 59\tAverage Score nan\n",
      "t 60\tAverage Score nan\n",
      "t 61\tAverage Score nan\n",
      "t 62\tAverage Score nan\n",
      "t 63\tAverage Score nan\n",
      "t 64\tAverage Score nan\n",
      "t 65\tAverage Score nan\n",
      "t 66\tAverage Score nan\n",
      "(tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([-0.5000, -0.1000,  0.2000, -0.7000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([-0.4000, -0.6000,  0.6000, -0.2000]), tensor([-0.7000, -0.9000, -0.9000, -0.8000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.4000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]), tensor([ 0.8000, -0.5000, -0.7000, -0.2000]))\n",
      "tensor([ 0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.4000, -0.5000, -0.1000,  0.2000, -0.7000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "        -0.4000, -0.6000,  0.6000, -0.2000, -0.7000, -0.9000, -0.9000, -0.8000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.4000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000,\n",
      "         0.8000, -0.5000, -0.7000, -0.2000,  0.8000, -0.5000, -0.7000, -0.2000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinlee/miniconda3/envs/pytorch/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:275: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'torch.Tensor'>\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/martinlee/miniconda3/envs/pytorch/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/martinlee/miniconda3/envs/pytorch/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m---> 45\u001b[0m scores \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m     46\u001b[0m plot_returns(scores)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#plot the scores\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m agent\u001b[38;5;241m.\u001b[39mstep(state, action, next_state, reward)\n\u001b[1;32m     32\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# For plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[1;32m     64\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample(BATCH_SIZE)\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(experiences, GAMMA)\n",
      "Cell \u001b[0;32mIn[5], line 122\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Compute Q(s_t, a) - the model computes Q(s_t)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# on the \"older\" target_net; selecting their best reward with max(1)[0].\u001b[39;00m\n\u001b[1;32m    121\u001b[0m q_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model(next_state_batch) \u001b[38;5;66;03m# shape: tensor (4,64,20)\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28mprint\u001b[39m(q_next\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    123\u001b[0m q_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m    124\u001b[0m q_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m20\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_space = env.observation_space, action_space = env.action_space, disc_class = 20, seed = 0)\n",
    "\n",
    "def train(n_episodes = 200, max_t = 1600):\n",
    "    \"\"\"\n",
    "    Training DQN\n",
    "    \n",
    "    @param n_episodes(int): Number of training episodes\n",
    "    @param max_t(int):      Maximum number of timesteps per episode\n",
    "    eps_start(float):       Starting value of epsilon for epsilon-greedy action selection\n",
    "    eps_end(float):         Minimum value of epsilon\n",
    "    eps_dec(float):         Multiplicative factor for decrementing epsilon\n",
    "    \"\"\"\n",
    "    # For plotting\n",
    "    scores = [ ]\n",
    "    scores_window = deque(maxlen=100) \n",
    "    for episode in range(1,n_episodes+1):\n",
    "        print('\\rEpisode {}'.format(episode), end=\"\\n\")\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.choose_action(state)\n",
    "            action = agent.disct2cont(action)\n",
    "            observation, reward, terminated, truncated, _  = env.step(action)\n",
    "            reward = torch.tensor([reward],device = device)\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            agent.step(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            \n",
    "            # For plotting\n",
    "            score += reward[0]\n",
    "            print('\\rt {}\\tAverage Score {:.2f}'.format(t,np.mean(scores_window)), end=\"\\n\")\n",
    "            if score > 300:\n",
    "                print(f\"Environment solved! t:{t}, episode:{episode}\")\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = train()\n",
    "plot_returns(scores)\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6c5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
