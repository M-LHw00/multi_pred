{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57ab2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://shorturl.at/nyzL4\n",
    "# https://shorturl.at/FO479\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598f76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\") #Setting up an existing open AI gym environment\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Represents a single transition in the environment. Used for experience replay. \"\"\"\n",
    "Experience = namedtuple('Experience',                                       \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory:\n",
    "    \"\"\"Fixed-size memory buffer to store Experience tuples.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Selects a random batch of transitions for training\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66147da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network model\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct DQN model\n",
    "        \n",
    "        @param state_size(int) : Dimensions of each state\n",
    "        @param action_size(int): Dimensions of each action\n",
    "        @param seed            : Random seed\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size,64)\n",
    "        self.fc2 = nn.Linear(64, 48)\n",
    "        self.fc3 = nn.Linear(48, 48)\n",
    "        self.fc4_1 = nn.Linear(48, action_size)\n",
    "        self.fc4_2 = nn.Linear(48, action_size)\n",
    "        self.fc4_3 = nn.Linear(48, action_size)\n",
    "        self.fc4_4 = nn.Linear(48, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward propagation of the network\n",
    "        \n",
    "        @param state: state\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred1 = self.fc4_1(x) # shape: (20,)\n",
    "        pred2 = self.fc4_2(x)\n",
    "        pred3 = self.fc4_3(x)\n",
    "        pred4 = self.fc4_4(x)\n",
    "        return pred1, pred2, pred3, pred4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfc20008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters (Maybe receive these as arguments? (parse_args))\"\"\"\n",
    "MEMORY_SIZE = int(5e4)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR = 5e-4\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.05\n",
    "EPS_DECAY = 1e-3\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent that interacts and learns from the environment.\n",
    "    Receives rewards from the environment, update DQN parameters, \"memorize\" experiences.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, disc_class, seed):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct an Agent object\n",
    "        \n",
    "        @param state_size(int) : Dimensions of each state\n",
    "        @param action_size(int): Dimensions of each action\n",
    "        @param seed            : Random seed\n",
    "        \"\"\"\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.disc_class = disc_class\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Initalize main and target DQN \n",
    "        self.main_model = DQN(state_space.shape[0], disc_class, seed).to(device)\n",
    "        self.target_model = DQN(state_space.shape[0], disc_class, seed).to(device)\n",
    "        self.target_model.load_state_dict(self.main_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.main_model.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY step)\n",
    "        self.t_step = 0\n",
    "        self.eps_step = 0\n",
    "        self.spacing = 0.1\n",
    "    \n",
    "    def add_memory(self, state, action, next_state, reward):\n",
    "        # save experience in replay memory\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Return action for given state following the current policy\n",
    "        \n",
    "        @param state: Current state\n",
    "        @param epsilon(float)  : epsilon for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        eps_threshold = EPSILON_MIN + (EPSILON - EPSILON_MIN) * math.exp(-1 * self.eps_step / EPS_DECAY)\n",
    "        self.eps_step += 1\n",
    "        # epsilon-greedy action selection\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                pred1, pred2, pred3, pred4 = self.main_model(state)\n",
    "                pred1 = np.argmax(pred1)\n",
    "                pred2 = np.argmax(pred2)\n",
    "                pred3 = np.argmax(pred3)\n",
    "                pred4 = np.argmax(pred4)\n",
    "                action = torch.tensor((pred1, pred2, pred3, pred4))\n",
    "                assert(0 <= pred1 and pred1 <= 19 and\n",
    "                       0 <= pred2 and pred2 <= 19 and\n",
    "                       0 <= pred3 and pred3 <= 19 and\n",
    "                       0 <= pred4 and pred4 <= 19)\n",
    "                # print(\"max:\",action, action.shape)\n",
    "                return action\n",
    "        else:\n",
    "            action = torch.randint(0, 20, (4,))\n",
    "            assert(0 <= action[0] and action[0] <= 19 and\n",
    "                   0 <= action[1] and action[1] <= 19 and\n",
    "                   0 <= action[2] and action[2] <= 19 and\n",
    "                   0 <= action[3] and action[3] <= 19)\n",
    "            # print(\"random:\", action, action.shape)\n",
    "            return action\n",
    "        \n",
    "    def disct2cont(self,action):\n",
    "        return action * self.spacing - self.action_space.high\n",
    "    \n",
    "    def cont2disc(self, action):\n",
    "        ''' converts continuous action valeus back to indices '''\n",
    "        res = [ ]\n",
    "        for i in range(len(action)):\n",
    "            res.append(pd.cut(action[i] , bins = np.linspace(-1.0, 1.0, num=21, endpoint = True), labels = np.arange(20), include_lowest=True, right= True))\n",
    "        return res\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update parameters using the given batch of expreience tuples\n",
    "        \n",
    "        @param experiences(Tuple): tuple of (s,a,s',r) tuples\n",
    "        @param gamma(float): discount factor\n",
    "        \"\"\"\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample(BATCH_SIZE)\n",
    "            batch = Experience(*zip(*experiences)) # This converts batch-array of Experiences\n",
    "                                                   # to Experience of batch-arrays.\n",
    "            #batch.action --> (batch_size, tensor(4,))               ==> (64, tensor(4,))\n",
    "            #batch.state --> (batch_size, tensor(1,state_size))      ==> (64, tensor(1,24))\n",
    "            #batch.next_state --> (batch_size, tensor(1,state_size)) ==> (64, tensor(1,24))\n",
    "            #batch.reward --> (batch_size, tensor(1))                ==> (64, tensor(1))\n",
    "            action_indicies = self.cont2disc(batch.action)\n",
    "            action_indicies_t = torch.tensor(list(zip(*action_indicies)))   # list(4,64)\n",
    "            state_batch = torch.cat(batch.state)           # tensor(64, state_size)\n",
    "            next_state_batch = torch.cat(batch.next_state) # tensor(64, state_size)\n",
    "            reward_batch = torch.cat(batch.reward) # shape (64, )\n",
    "            rewards = reward_batch.repeat(4,1) # shape: tensor(4,64)\n",
    "\n",
    "            self.main_model.train() # Set local network as training mode\n",
    "            self.target_model.eval() # Set target network as evaluation mode\n",
    "\n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t)\n",
    "            state_values = torch.stack(list(self.main_model(state_batch)), dim = 0) # changing tuple to tensor shape: tensor(4,64,20)\n",
    "            state_action_values = torch.gather(state_values, 2, action_indicies_t.unsqueeze(-1)) # extracting q_values based on actions\n",
    "            q_eval = torch.squeeze(state_action_values)\n",
    "\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            # Expected values of actions for non_final_next_states are computed based\n",
    "            # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "            q_next = self.target_model(next_state_batch) # shape: tuple (4, tensor(64,20))\n",
    "            q_next_max = torch.zeros(4,64)\n",
    "            for i in range(len(q_next_max)):\n",
    "                q_next_max[i] = torch.amax(q_next[i],dim=1)\n",
    "            q_target = rewards + GAMMA * q_next_max # (4,64)\n",
    "            loss = F.smooth_l1_loss(q_eval, q_target)\n",
    "            print(\"before backward\",loss)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            print(\"after backward\",loss)\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        \"\"\" UPDATE TARGET NETWORK Don't update every step (compare) \"\"\" \n",
    "        self.soft_update(self.main_model, self.target_model, TAU)\n",
    "    \n",
    "    def soft_update(self, main_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \n",
    "        @param local_model(nn model): weights will be copied from\n",
    "        @param target_model(nn model): weights will be copied to\n",
    "        @param tau(float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, main_param in zip(target_model.parameters(), main_model.parameters()):\n",
    "            target_param.data.copy_(tau*main_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab2b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(returns, window=10):\n",
    "    '''\n",
    "    Returns (iterable): list of returns over time\n",
    "    window: window for rolling mean to smooth plotted curve\n",
    "    '''\n",
    "    sns.lineplot(\n",
    "        data=pd.DataFrame(returns).rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cfcfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "before backward tensor(0.0030, dtype=torch.float64, grad_fn=<SmoothL1LossBackward0>)\n",
      "after backward tensor(0.0030, dtype=torch.float64, grad_fn=<SmoothL1LossBackward0>)\n",
      "Average Score -103.71\n",
      "Episode 2\n",
      "before backward tensor(0.0024, dtype=torch.float64, grad_fn=<SmoothL1LossBackward0>)\n",
      "after backward tensor(0.0024, dtype=torch.float64, grad_fn=<SmoothL1LossBackward0>)\n",
      "Average Score -104.83\n",
      "Episode 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[0;32m---> 46\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m plot_returns(scores)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#plot the scores\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     23\u001b[0m index \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m     24\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdisct2cont(index)\n\u001b[0;32m---> 26\u001b[0m next_observation, reward, done, truncated, _  \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     29\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward],device \u001b[38;5;241m=\u001b[39m device)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/envs/box2d/bipedal_walker.py:548\u001b[0m, in \u001b[0;36mBipedalWalker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mmotorSpeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(SPEED_KNEE \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msign(action[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mmaxMotorTorque \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\n\u001b[1;32m    545\u001b[0m         MOTORS_TORQUE \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(np\u001b[38;5;241m.\u001b[39mabs(action[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    546\u001b[0m     )\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhull\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    551\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhull\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/envs/box2d/bipedal_walker.py:88\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     85\u001b[0m     contactListener\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mhull \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureA\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mhull \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureB\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     92\u001b[0m     ):\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_space = env.observation_space, action_space = env.action_space, disc_class = 20, seed = 0)\n",
    "\n",
    "def train(n_episodes = 10000, max_t = 1600):\n",
    "    \"\"\"\n",
    "    Training DQN\n",
    "    \n",
    "    @param n_episodes(int): Number of training episodes\n",
    "    @param max_t(int):      Maximum number of timesteps per episode\n",
    "    eps_start(float):       Starting value of epsilon for epsilon-greedy action selection\n",
    "    eps_end(float):         Minimum value of epsilon\n",
    "    eps_dec(float):         Multiplicative factor for decrementing epsilon\n",
    "    \"\"\"\n",
    "    # For plotting\n",
    "    scores = [ ]\n",
    "    scores_window = deque(maxlen=100) \n",
    "    for episode in range(1,n_episodes+1):\n",
    "        print('\\rEpisode {}'.format(episode), end=\"\\n\")\n",
    "        observation, info = env.reset()\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        score = 0\n",
    "        done = False\n",
    "        for t in range(max_t):\n",
    "            index = agent.choose_action(state)\n",
    "            action = agent.disct2cont(index)\n",
    "            \n",
    "            next_observation, reward, done, truncated, _  = env.step(action)\n",
    "            next_state = torch.tensor(next_observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            reward = torch.tensor([reward],device = device)\n",
    "            score += reward[0]\n",
    "            agent.add_memory(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        agent.learn()\n",
    "        # For plotting\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rAverage Score {:.2f}'.format(np.mean(scores_window)), end=\"\\n\")\n",
    "        if score > 300:\n",
    "            print(f\"Environment solved! t:{t}, episode:{episode}\")\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = train()\n",
    "plot_returns(scores)\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106b548-14be-4c09-bf55-73b146ee51c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
