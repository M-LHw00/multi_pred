{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57ab2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://shorturl.at/nyzL4\n",
    "# https://shorturl.at/FO479\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598f76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\") #Setting up an existing open AI gym environment\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "    plt.ion()\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # GPU acceleration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Represents a single transition in the environment. Used for experience replay. \"\"\"\n",
    "Experience = namedtuple('Experience',                                       \n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory:\n",
    "    \"\"\"Fixed-size memory buffer to store Experience tuples.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Selects a random batch of transitions for training\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66147da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network model\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct DQN model\n",
    "        \n",
    "        @param state_size(int) : Dimensions of each state\n",
    "        @param action_size(int): Dimensions of each action\n",
    "        @param seed            : Random seed\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size,64)\n",
    "        self.fc2 = nn.Linear(64, 48)\n",
    "        self.fc3 = nn.Linear(48, 48)\n",
    "        self.fc4_1 = nn.Linear(48, action_size)\n",
    "        self.fc4_2 = nn.Linear(48, action_size)\n",
    "        self.fc4_3 = nn.Linear(48, action_size)\n",
    "        self.fc4_4 = nn.Linear(48, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward propagation of the network\n",
    "        \n",
    "        @param state: state\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred1 = self.fc4_1(x) # shape: (20,)\n",
    "        pred2 = self.fc4_2(x)\n",
    "        pred3 = self.fc4_3(x)\n",
    "        pred4 = self.fc4_4(x)\n",
    "        return pred1, pred2, pred3, pred4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc20008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters (Maybe receive these as arguments? (parse_args))\"\"\"\n",
    "MEMORY_SIZE = int(5e4)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR = 5e-4\n",
    "EPSILON = 1.0\n",
    "EPSILON_MIN = 0.05\n",
    "EPS_DECAY = 1e-3\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    Agent that interacts and learns from the environment.\n",
    "    Receives rewards from the environment, update DQN parameters, \"memorize\" experiences.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space, disc_class, seed):\n",
    "        \"\"\"\n",
    "        Initialize parameters and construct an Agent object\n",
    "        \n",
    "        @param state_size(int) : Dimensions of each state\n",
    "        @param action_size(int): Dimensions of each action\n",
    "        @param seed            : Random seed\n",
    "        \"\"\"\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.disc_class = disc_class\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        # Initalize main and target DQN \n",
    "        self.main_model = DQN(state_space.shape[0], disc_class, seed).to(device)\n",
    "        self.target_model = DQN(state_space.shape[0], disc_class, seed).to(device)\n",
    "        self.target_model.load_state_dict(self.main_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.main_model.parameters(), lr=LR)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY step)\n",
    "        self.t_step = 0\n",
    "        self.eps_step = 0\n",
    "        self.spacing = 0.1\n",
    "    \n",
    "    def add_memory(self, state, action, next_state, reward):\n",
    "        # save experience in replay memory\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Return action for given state following the current policy\n",
    "        \n",
    "        @param state: Current state\n",
    "        @param epsilon(float)  : epsilon for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        eps_threshold = EPSILON_MIN + (EPSILON - EPSILON_MIN) * math.exp(-1 * self.eps_step / EPS_DECAY)\n",
    "        self.eps_step += 1\n",
    "        # epsilon-greedy action selection\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                pred1, pred2, pred3, pred4 = self.main_model(state)\n",
    "                pred1 = np.argmax(pred1)\n",
    "                pred2 = np.argmax(pred2)\n",
    "                pred3 = np.argmax(pred3)\n",
    "                pred4 = np.argmax(pred4)\n",
    "                action = torch.tensor((pred1, pred2, pred3, pred4))\n",
    "                assert(0 <= pred1 and pred1 <= 19 and\n",
    "                       0 <= pred2 and pred2 <= 19 and\n",
    "                       0 <= pred3 and pred3 <= 19 and\n",
    "                       0 <= pred4 and pred4 <= 19)\n",
    "                # print(\"max:\",action, action.shape)\n",
    "                return action\n",
    "        else:\n",
    "            action = torch.randint(0, 20, (4,))\n",
    "            assert(0 <= action[0] and action[0] <= 19 and\n",
    "                   0 <= action[1] and action[1] <= 19 and\n",
    "                   0 <= action[2] and action[2] <= 19 and\n",
    "                   0 <= action[3] and action[3] <= 19)\n",
    "            # print(\"random:\", action, action.shape)\n",
    "            return action\n",
    "        \n",
    "    def disct2cont(self,action):\n",
    "        return action * self.spacing - self.action_space.high\n",
    "    \n",
    "    def cont2disc(self, action):\n",
    "        ''' converts continuous action valeus back to indices '''\n",
    "        res = [ ]\n",
    "        for i in range(len(action)):\n",
    "            res.append(pd.cut(action[i] , bins = np.linspace(-1.0, 1.0, num=21, endpoint = True), labels = np.arange(20), include_lowest=True, right= True))\n",
    "        return res\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update parameters using the given batch of expreience tuples\n",
    "        \n",
    "        @param experiences(Tuple): tuple of (s,a,s',r) tuples\n",
    "        @param gamma(float): discount factor\n",
    "        \"\"\"\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample(BATCH_SIZE)\n",
    "            batch = Experience(*zip(*experiences)) # This converts batch-array of Experiences\n",
    "                                                   # to Experience of batch-arrays.\n",
    "            #batch.action --> (batch_size, tensor(4,))               ==> (64, tensor(4,))\n",
    "            #batch.state --> (batch_size, tensor(1,state_size))      ==> (64, tensor(1,24))\n",
    "            #batch.next_state --> (batch_size, tensor(1,state_size)) ==> (64, tensor(1,24))\n",
    "            #batch.reward --> (batch_size, tensor(1))                ==> (64, tensor(1))\n",
    "            action_indicies = self.cont2disc(batch.action)\n",
    "            action_indicies_t = torch.tensor(list(zip(*action_indicies)))   # list(4,64)\n",
    "            state_batch = torch.cat(batch.state)           # tensor(64, state_size)\n",
    "            next_state_batch = torch.cat(batch.next_state) # tensor(64, state_size)\n",
    "            reward_batch = torch.cat(batch.reward) # shape (64, )\n",
    "            rewards = reward_batch.repeat(4,1) # shape: tensor(4,64)\n",
    "\n",
    "            self.main_model.train() # Set local network as training mode\n",
    "            self.target_model.eval() # Set target network as evaluation mode\n",
    "\n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t)\n",
    "            state_values = torch.stack(list(self.main_model(state_batch)), dim = 0) # changing tuple to tensor shape: tensor(4,64,20)\n",
    "            state_action_values = torch.gather(state_values, 2, action_indicies_t.unsqueeze(-1)) # extracting q_values based on actions\n",
    "            q_eval = torch.squeeze(state_action_values)\n",
    "\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            # Expected values of actions for non_final_next_states are computed based\n",
    "            # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "            q_next = self.target_model(next_state_batch) # shape: tuple (4, tensor(64,20))\n",
    "            q_next_max = torch.zeros(4,64)\n",
    "            for i in range(len(q_next_max)):\n",
    "                q_next_max[i] = torch.amax(q_next[i],dim=1)\n",
    "            q_target = rewards + GAMMA * q_next_max # (4,64)\n",
    "            loss = F.smooth_l1_loss(q_eval, q_target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_value = float(loss.item())\n",
    "            return loss_value\n",
    "        \n",
    "        \"\"\" UPDATE TARGET NETWORK Don't update every step (compare) \"\"\" \n",
    "        self.soft_update(self.main_model, self.target_model, TAU)\n",
    "    \n",
    "    def soft_update(self, main_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \n",
    "        @param local_model(nn model): weights will be copied from\n",
    "        @param target_model(nn model): weights will be copied to\n",
    "        @param tau(float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, main_param in zip(target_model.parameters(), main_model.parameters()):\n",
    "            target_param.data.copy_(tau*main_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2b218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(returns, window=10):\n",
    "    '''\n",
    "    Returns (iterable): list of returns over time\n",
    "    window: window for rolling mean to smooth plotted curve\n",
    "    '''\n",
    "    sns.lineplot(\n",
    "        data=pd.DataFrame(returns).rolling(window=window).mean()[window-1::window]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cfcfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "loss: 0.0032777779743097925\n",
      "Average Score -103.75\n",
      "Episode 2\n",
      "loss: 0.0024023394892315813\n",
      "Average Score -104.49\n",
      "Episode 3\n",
      "loss: 0.002677591782597957\n",
      "Average Score -104.40\n",
      "Episode 4\n",
      "loss: 0.0026196526351995424\n",
      "Average Score -102.72\n",
      "Episode 5\n",
      "loss: 0.0022826031850569197\n",
      "Average Score -100.38\n",
      "Episode 6\n",
      "loss: 0.0029727621404549067\n",
      "Average Score -101.10\n",
      "Episode 7\n",
      "loss: 0.004688799599822998\n",
      "Average Score -102.19\n",
      "Episode 8\n",
      "loss: 0.003223184270463521\n",
      "Average Score -102.06\n",
      "Episode 9\n",
      "loss: 0.0033477306133775373\n",
      "Average Score -102.04\n",
      "Episode 10\n",
      "loss: 0.0019651653351435523\n",
      "Average Score -101.91\n",
      "Episode 11\n",
      "loss: 1.5556427125442152\n",
      "Average Score -101.80\n",
      "Episode 12\n",
      "loss: 0.003659826283658926\n",
      "Average Score -101.70\n",
      "Episode 13\n",
      "loss: 0.004194974392972407\n",
      "Average Score -101.62\n",
      "Episode 14\n",
      "loss: 0.003711328138848305\n",
      "Average Score -101.60\n",
      "Episode 15\n",
      "loss: 0.004319792325890033\n",
      "Average Score -101.50\n",
      "Episode 16\n",
      "loss: 0.005841563910450471\n",
      "Average Score -100.78\n",
      "Episode 17\n",
      "loss: 0.005058961044543056\n",
      "Average Score -101.27\n",
      "Episode 18\n",
      "loss: 0.004246746765375639\n",
      "Average Score -101.95\n",
      "Episode 19\n",
      "loss: 0.005542963736639657\n",
      "Average Score -101.81\n",
      "Episode 20\n",
      "loss: 0.0039435478806470045\n",
      "Average Score -103.01\n",
      "Episode 21\n",
      "loss: 0.003470677414744335\n",
      "Average Score -103.51\n",
      "Episode 22\n",
      "loss: 0.005317960991176095\n",
      "Average Score -103.91\n",
      "Episode 23\n",
      "loss: 0.005212822245971553\n",
      "Average Score -104.49\n",
      "Episode 24\n",
      "loss: 0.0037366781814132484\n",
      "Average Score -105.06\n",
      "Episode 25\n",
      "loss: 0.007293156979002976\n",
      "Average Score -105.38\n",
      "Episode 26\n",
      "loss: 0.006272861904689181\n",
      "Average Score -105.74\n",
      "Episode 27\n",
      "loss: 0.004378654142662827\n",
      "Average Score -106.18\n",
      "Episode 28\n",
      "loss: 0.0033255495358377697\n",
      "Average Score -106.67\n",
      "Episode 29\n",
      "loss: 0.006718665570880704\n",
      "Average Score -106.97\n",
      "Episode 30\n",
      "loss: 0.0052991463385846285\n",
      "Average Score -107.21\n",
      "Episode 31\n",
      "loss: 3.117528655206065\n",
      "Average Score -107.35\n",
      "Episode 32\n",
      "loss: 1.5618997139327222\n",
      "Average Score -107.16\n",
      "Episode 33\n",
      "loss: 3.11298446011442\n",
      "Average Score -107.18\n",
      "Episode 34\n",
      "loss: 0.003989978137318662\n",
      "Average Score -107.13\n",
      "Episode 35\n",
      "loss: 0.006987164551101393\n",
      "Average Score -107.13\n",
      "Episode 36\n",
      "loss: 0.005647710394405472\n",
      "Average Score -107.08\n",
      "Episode 37\n",
      "loss: 0.0039069898736939475\n",
      "Average Score -106.98\n",
      "Episode 38\n",
      "loss: 0.0061838230314990115\n",
      "Average Score -106.97\n",
      "Episode 39\n",
      "loss: 0.0033969138771920582\n",
      "Average Score -106.98\n",
      "Episode 40\n",
      "loss: 0.0050682713683503555\n",
      "Average Score -107.01\n",
      "Episode 41\n",
      "loss: 0.006365650805497753\n",
      "Average Score -107.00\n",
      "Episode 42\n",
      "loss: 1.559946514911647\n",
      "Average Score -106.86\n",
      "Episode 43\n",
      "loss: 0.006027949867567395\n",
      "Average Score -106.79\n",
      "Episode 44\n",
      "loss: 1.5616153894995008\n",
      "Average Score -106.63\n",
      "Episode 45\n",
      "loss: 0.002430211915384212\n",
      "Average Score -106.49\n",
      "Episode 46\n",
      "loss: 0.0031753263668680416\n",
      "Average Score -106.34\n",
      "Episode 47\n",
      "loss: 0.004487940598935851\n",
      "Average Score -106.24\n",
      "Episode 48\n",
      "loss: 0.003358149207915065\n",
      "Average Score -106.11\n",
      "Episode 49\n",
      "loss: 0.0027619523735330823\n",
      "Average Score -106.08\n",
      "Episode 50\n",
      "loss: 0.003401007459053151\n",
      "Average Score -105.51\n",
      "Episode 51\n",
      "loss: 0.0038479660442276315\n",
      "Average Score -105.41\n",
      "Episode 52\n",
      "loss: 0.005585294767425924\n",
      "Average Score -104.88\n",
      "Episode 53\n",
      "loss: 0.0033860158623428157\n",
      "Average Score -104.38\n",
      "Episode 54\n",
      "loss: 0.0035393408263208782\n",
      "Average Score -104.28\n",
      "Episode 55\n",
      "loss: 0.005044431507295281\n",
      "Average Score -103.81\n",
      "Episode 56\n",
      "loss: 0.004190595260255197\n",
      "Average Score -103.79\n",
      "Episode 57\n",
      "loss: 0.002777878653460088\n",
      "Average Score -103.73\n",
      "Episode 58\n",
      "loss: 0.0030453763222128938\n",
      "Average Score -104.00\n",
      "Episode 59\n",
      "loss: 0.003342716096025162\n",
      "Average Score -103.90\n",
      "Episode 60\n",
      "loss: 0.004373661897627147\n",
      "Average Score -103.80\n",
      "Episode 61\n",
      "loss: 0.0053602395130830805\n",
      "Average Score -104.10\n",
      "Episode 62\n",
      "loss: 0.0039132194018751615\n",
      "Average Score -104.00\n",
      "Episode 63\n",
      "loss: 0.007074931437716267\n",
      "Average Score -103.91\n",
      "Episode 64\n",
      "loss: 1.5557819827923913\n",
      "Average Score -103.92\n",
      "Episode 65\n",
      "loss: 0.003591366096118542\n",
      "Average Score -103.89\n",
      "Episode 66\n",
      "loss: 0.002992901258910815\n",
      "Average Score -103.79\n",
      "Episode 67\n",
      "loss: 0.004906172983356528\n",
      "Average Score -103.68\n",
      "Episode 68\n",
      "loss: 0.002453272606922015\n",
      "Average Score -103.82\n",
      "Episode 69\n",
      "loss: 1.5556402521092243\n",
      "Average Score -104.04\n",
      "Episode 70\n",
      "loss: 0.005302487772756491\n",
      "Average Score -104.21\n",
      "Episode 71\n",
      "loss: 0.004610488860139129\n",
      "Average Score -104.11\n",
      "Episode 72\n",
      "loss: 0.003605499000945103\n",
      "Average Score -104.03\n",
      "Episode 73\n",
      "loss: 0.003897447270354379\n",
      "Average Score -104.23\n",
      "Episode 74\n",
      "loss: 0.0019125245978875466\n",
      "Average Score -104.34\n",
      "Episode 75\n",
      "loss: 1.5585876857980472\n",
      "Average Score -104.58\n",
      "Episode 76\n",
      "loss: 0.004084104810800786\n",
      "Average Score -104.81\n",
      "Episode 77\n",
      "loss: 0.003609145720161083\n",
      "Average Score -105.04\n",
      "Episode 78\n",
      "loss: 1.5574620544786077\n",
      "Average Score -105.27\n",
      "Episode 79\n",
      "loss: 0.005100934474019213\n",
      "Average Score -105.48\n",
      "Episode 80\n",
      "loss: 1.5552209184764516\n",
      "Average Score -105.73\n",
      "Episode 81\n",
      "loss: 0.007262702861669898\n",
      "Average Score -105.95\n",
      "Episode 82\n",
      "loss: 0.004603682464131297\n",
      "Average Score -106.18\n",
      "Episode 83\n",
      "loss: 0.002984530240929041\n",
      "Average Score -106.11\n",
      "Episode 84\n",
      "loss: 1.5588327103524178\n",
      "Average Score -106.04\n",
      "Episode 85\n",
      "loss: 0.003851298728434876\n",
      "Average Score -105.95\n",
      "Episode 86\n",
      "loss: 0.0030282545822572114\n",
      "Average Score -105.63\n",
      "Episode 87\n",
      "loss: 0.0036128430045171242\n",
      "Average Score -105.57\n",
      "Episode 88\n",
      "loss: 0.0024694842506625147\n",
      "Average Score -105.50\n",
      "Episode 89\n",
      "loss: 1.5576580093552346\n",
      "Average Score -105.42\n",
      "Episode 90\n",
      "loss: 0.004377639686304748\n",
      "Average Score -105.05\n",
      "Episode 91\n",
      "loss: 0.00905079485113387\n",
      "Average Score -104.68\n",
      "Episode 92\n",
      "loss: 0.0031187163481572492\n",
      "Average Score -104.34\n",
      "Episode 93\n",
      "loss: 0.008468609603629482\n",
      "Average Score -104.28\n",
      "Episode 94\n",
      "loss: 0.004940309810397726\n",
      "Average Score -104.23\n",
      "Episode 95\n",
      "loss: 0.005119341565575922\n",
      "Average Score -104.19\n",
      "Episode 96\n",
      "loss: 0.003954968725294194\n",
      "Average Score -104.15\n",
      "Episode 97\n",
      "loss: 0.0024529864476828386\n",
      "Average Score -103.80\n",
      "Episode 98\n",
      "loss: 0.0020001841876707504\n",
      "Average Score -103.77\n",
      "Episode 99\n",
      "loss: 0.0014998713151620439\n",
      "Average Score -103.95\n",
      "Episode 100\n",
      "loss: 0.004255785014047877\n",
      "Average Score -103.91\n",
      "Episode 101\n",
      "loss: 3.114340844763554\n",
      "Average Score -103.86\n",
      "Episode 102\n",
      "loss: 0.004659741393566521\n",
      "Average Score -104.01\n",
      "Episode 103\n",
      "loss: 0.0034745877105021817\n",
      "Average Score -104.21\n",
      "Episode 104\n",
      "loss: 0.00398616809533247\n",
      "Average Score -104.47\n",
      "Episode 105\n",
      "loss: 0.003960126607449708\n",
      "Average Score -104.13\n",
      "Episode 106\n",
      "loss: 1.5586114002396196\n",
      "Average Score -104.10\n",
      "Episode 107\n",
      "loss: 0.0026845959072250318\n",
      "Average Score -104.24\n",
      "Episode 108\n",
      "loss: 0.005509542957763569\n",
      "Average Score -104.45\n",
      "Episode 109\n",
      "loss: 0.004018490951210693\n",
      "Average Score -104.69\n",
      "Episode 110\n",
      "loss: 0.0022274538315736293\n",
      "Average Score -104.66\n",
      "Episode 111\n",
      "loss: 1.557314516677995\n",
      "Average Score -104.89\n",
      "Episode 112\n",
      "loss: 0.004450495657035482\n",
      "Average Score -105.11\n",
      "Episode 113\n",
      "loss: 1.558809416599983\n",
      "Average Score -105.12\n",
      "Episode 114\n",
      "loss: 0.0036393218748636295\n",
      "Average Score -104.80\n",
      "Episode 115\n",
      "loss: 0.0052345510681762394\n",
      "Average Score -105.04\n",
      "Episode 116\n",
      "loss: 0.0013318957862197699\n",
      "Average Score -105.13\n",
      "Episode 117\n",
      "loss: 0.006680231227487404\n",
      "Average Score -105.03\n",
      "Episode 118\n",
      "loss: 0.002544489061536428\n",
      "Average Score -105.12\n",
      "Episode 119\n",
      "loss: 0.0025882366753784965\n",
      "Average Score -105.12\n",
      "Episode 120\n",
      "loss: 0.005035859952063588\n",
      "Average Score -104.39\n",
      "Episode 121\n",
      "loss: 0.003974102089553304\n",
      "Average Score -104.24\n",
      "Episode 122\n",
      "loss: 0.004133138510652892\n",
      "Average Score -104.10\n",
      "Episode 123\n",
      "loss: 0.006880698352938191\n",
      "Average Score -103.93\n",
      "Episode 124\n",
      "loss: 0.0024086426807411196\n",
      "Average Score -103.74\n",
      "Episode 125\n",
      "loss: 0.0058955481097722\n",
      "Average Score -103.62\n",
      "Episode 126\n",
      "loss: 0.003048732828508955\n",
      "Average Score -103.45\n",
      "Episode 127\n",
      "loss: 0.001675567895044351\n",
      "Average Score -102.95\n",
      "Episode 128\n",
      "loss: 0.0032026262720436056\n",
      "Average Score -102.73\n",
      "Episode 129\n",
      "loss: 0.002076386313697726\n",
      "Average Score -102.25\n",
      "Episode 130\n",
      "loss: 1.5564550351774644\n",
      "Average Score -102.11\n",
      "Episode 131\n",
      "loss: 0.0022006617567287175\n",
      "Average Score -101.98\n",
      "Episode 132\n",
      "loss: 0.0017131604841132838\n",
      "Average Score -101.95\n",
      "Episode 133\n",
      "loss: 0.0025366039383074137\n",
      "Average Score -101.82\n",
      "Episode 134\n",
      "loss: 1.5563906934149114\n",
      "Average Score -101.76\n",
      "Episode 135\n",
      "loss: 0.002846841159006572\n",
      "Average Score -101.66\n",
      "Episode 136\n",
      "loss: 1.5553873136414347\n",
      "Average Score -101.60\n",
      "Episode 137\n",
      "loss: 0.004331659425856787\n",
      "Average Score -101.53\n",
      "Episode 138\n",
      "loss: 0.004668729926788462\n",
      "Average Score -101.46\n",
      "Episode 139\n",
      "loss: 0.0015007863442683888\n",
      "Average Score -101.37\n",
      "Episode 140\n",
      "loss: 1.5563234543614313\n",
      "Average Score -101.49\n",
      "Episode 141\n",
      "loss: 0.0032644285184823675\n",
      "Average Score -101.55\n",
      "Episode 142\n",
      "loss: 0.005033326149739226\n",
      "Average Score -101.53\n",
      "Episode 143\n",
      "loss: 1.5589042279162697\n",
      "Average Score -101.72\n",
      "Episode 144\n",
      "loss: 0.004006359078189959\n",
      "Average Score -101.99\n",
      "Episode 145\n",
      "loss: 0.006249656334231426\n",
      "Average Score -101.98\n",
      "Episode 146\n",
      "loss: 0.0013866555210856168\n",
      "Average Score -101.99\n",
      "Episode 147\n",
      "loss: 0.005909958087505923\n",
      "Average Score -102.21\n",
      "Episode 148\n",
      "loss: 0.002256904649692187\n",
      "Average Score -102.40\n",
      "Episode 149\n",
      "loss: 0.004230064055611181\n",
      "Average Score -102.62\n",
      "Episode 150\n",
      "loss: 0.002532012456945393\n",
      "Average Score -103.07\n",
      "Episode 151\n",
      "loss: 0.0020577215001288595\n",
      "Average Score -103.27\n",
      "Episode 152\n",
      "loss: 0.0026992265867039553\n",
      "Average Score -103.67\n",
      "Episode 153\n",
      "loss: 0.002105265329862654\n",
      "Average Score -104.06\n",
      "Episode 154\n",
      "loss: 0.0019544054719481674\n",
      "Average Score -104.13\n",
      "Episode 155\n",
      "loss: 0.004146827270660884\n",
      "Average Score -104.56\n",
      "Episode 156\n",
      "loss: 0.004791468768566242\n",
      "Average Score -104.74\n",
      "Episode 157\n",
      "loss: 0.0016343543261421929\n",
      "Average Score -104.99\n",
      "Episode 158\n",
      "loss: 0.0036918113942032175\n",
      "Average Score -104.79\n",
      "Episode 159\n",
      "loss: 1.5586764095130852\n",
      "Average Score -104.82\n",
      "Episode 160\n",
      "loss: 0.0031498102204205276\n",
      "Average Score -105.07\n",
      "Episode 161\n",
      "loss: 0.0029145347621444516\n",
      "Average Score -104.87\n",
      "Episode 162\n",
      "loss: 0.0030956178933412735\n",
      "Average Score -104.91\n",
      "Episode 163\n",
      "loss: 1.559084593918772\n",
      "Average Score -104.95\n",
      "Episode 164\n",
      "loss: 0.0036301302114196492\n",
      "Average Score -104.92\n",
      "Episode 165\n",
      "loss: 0.0035794879413918743\n",
      "Average Score -104.92\n",
      "Episode 166\n",
      "loss: 0.003607574973689086\n",
      "Average Score -104.97\n",
      "Episode 167\n",
      "loss: 1.5557528366950046\n",
      "Average Score -105.03\n",
      "Episode 168\n",
      "loss: 0.004394050865645138\n",
      "Average Score -104.90\n",
      "Episode 169\n",
      "loss: 0.003432557987508548\n",
      "Average Score -104.74\n",
      "Episode 170\n",
      "loss: 0.00243578224806625\n",
      "Average Score -104.60\n",
      "Episode 171\n",
      "loss: 0.002020220396492116\n",
      "Average Score -104.63\n",
      "Episode 172\n",
      "loss: 0.002678747666717313\n",
      "Average Score -104.84\n",
      "Episode 173\n",
      "loss: 0.0019926387311844955\n",
      "Average Score -104.67\n",
      "Episode 174\n",
      "loss: 0.0029603156185655723\n",
      "Average Score -104.57\n",
      "Episode 175\n",
      "loss: 0.005752067569087444\n",
      "Average Score -104.38\n",
      "Episode 176\n",
      "loss: 0.002420403585636684\n",
      "Average Score -104.18\n",
      "Episode 177\n",
      "loss: 0.0022622121311217224\n",
      "Average Score -103.98\n",
      "Episode 178\n",
      "loss: 0.0028576543426306203\n",
      "Average Score -103.78\n",
      "Episode 179\n",
      "loss: 0.0013373253924638666\n",
      "Average Score -103.59\n",
      "Episode 180\n",
      "loss: 3.109618104057006\n",
      "Average Score -103.37\n",
      "Episode 181\n",
      "loss: 0.0023105856231656896\n",
      "Average Score -103.19\n",
      "Episode 182\n",
      "loss: 0.0029549046998967685\n",
      "Average Score -102.93\n",
      "Episode 183\n",
      "loss: 0.0034706799699919934\n",
      "Average Score -102.97\n",
      "Episode 184\n",
      "loss: 1.55357617041112\n",
      "Average Score -103.01\n",
      "Episode 185\n",
      "loss: 1.5549632029585918\n",
      "Average Score -103.27\n",
      "Episode 186\n",
      "loss: 0.004319625745674531\n",
      "Average Score -103.48\n",
      "Episode 187\n",
      "loss: 0.0013903048549317213\n",
      "Average Score -103.47\n",
      "Episode 188\n",
      "loss: 0.0026824480777928135\n",
      "Average Score -103.76\n",
      "Episode 189\n",
      "loss: 0.0024785526420730633\n",
      "Average Score -103.77\n",
      "Episode 190\n",
      "loss: 0.006358162167929265\n",
      "Average Score -104.39\n",
      "Episode 191\n",
      "loss: 0.008148835340673967\n",
      "Average Score -104.97\n",
      "Episode 192\n",
      "loss: 1.5556206173599878\n",
      "Average Score -105.58\n",
      "Episode 193\n",
      "loss: 0.004939411594776805\n",
      "Average Score -105.59\n",
      "Episode 194\n",
      "loss: 0.003310217529535639\n",
      "Average Score -105.59\n",
      "Episode 195\n",
      "loss: 1.554869134059195\n",
      "Average Score -105.86\n",
      "Episode 196\n",
      "loss: 0.005456119597631664\n",
      "Average Score -105.90\n",
      "Episode 197\n",
      "loss: 0.003136579052827357\n",
      "Average Score -106.19\n",
      "Episode 198\n",
      "loss: 0.00239151823514576\n",
      "Average Score -106.22\n",
      "Episode 199\n",
      "loss: 0.004776458442187819\n",
      "Average Score -105.99\n",
      "Episode 200\n",
      "loss: 1.5553646260547302\n",
      "Average Score -106.03\n",
      "Episode 201\n",
      "loss: 0.005405052148811016\n",
      "Average Score -106.05\n",
      "Episode 202\n",
      "loss: 0.003591097290758675\n",
      "Average Score -105.87\n",
      "Episode 203\n",
      "loss: 0.0025037464495607197\n",
      "Average Score -105.66\n",
      "Episode 204\n",
      "loss: 0.0045422542371411105\n",
      "Average Score -105.44\n",
      "Episode 205\n",
      "loss: 0.0029068772341696815\n",
      "Average Score -105.90\n",
      "Episode 206\n",
      "loss: 0.0012969021912607025\n",
      "Average Score -105.90\n",
      "Episode 207\n",
      "loss: 0.0032862718516911914\n",
      "Average Score -105.70\n",
      "Episode 208\n",
      "loss: 0.002554430486880856\n",
      "Average Score -105.50\n",
      "Episode 209\n",
      "loss: 0.0038184440076558953\n",
      "Average Score -105.24\n",
      "Episode 210\n",
      "loss: 0.004055409888170261\n",
      "Average Score -105.27\n",
      "Episode 211\n",
      "loss: 0.0017340159275876096\n",
      "Average Score -105.05\n",
      "Episode 212\n",
      "loss: 0.004327536338505537\n",
      "Average Score -104.85\n",
      "Episode 213\n",
      "loss: 0.002994201648739151\n",
      "Average Score -104.86\n",
      "Episode 214\n",
      "loss: 0.0042945111579958855\n",
      "Average Score -105.20\n",
      "Episode 215\n",
      "loss: 1.554699016971811\n",
      "Average Score -104.96\n",
      "Episode 216\n",
      "loss: 0.0032341867751904496\n",
      "Average Score -104.95\n",
      "Episode 217\n",
      "loss: 0.0019857164552190085\n",
      "Average Score -104.99\n",
      "Episode 218\n",
      "loss: 0.005212493290778102\n",
      "Average Score -104.77\n",
      "Episode 219\n",
      "loss: 0.004105506370810773\n",
      "Average Score -104.77\n",
      "Episode 220\n",
      "loss: 0.005158312876330031\n",
      "Average Score -105.24\n",
      "Episode 221\n",
      "loss: 0.004773974136633098\n",
      "Average Score -105.28\n",
      "Episode 222\n",
      "loss: 0.0026246958928661567\n",
      "Average Score -105.30\n",
      "Episode 223\n",
      "loss: 1.5572365369192647\n",
      "Average Score -105.54\n",
      "Episode 224\n",
      "loss: 0.0027493533575383674\n",
      "Average Score -105.56\n",
      "Episode 225\n",
      "loss: 1.5529755574910198\n",
      "Average Score -105.56\n",
      "Episode 226\n",
      "loss: 0.002572841357732231\n",
      "Average Score -105.64\n",
      "Episode 227\n",
      "loss: 0.0024686701716324236\n",
      "Average Score -106.14\n",
      "Episode 228\n",
      "loss: 4.661465484886393\n",
      "Average Score -106.17\n",
      "Episode 229\n",
      "loss: 0.003931586642833252\n",
      "Average Score -106.50\n",
      "Episode 230\n",
      "loss: 0.0016554065639101238\n",
      "Average Score -106.53\n",
      "Episode 231\n",
      "loss: 0.002831167782067213\n",
      "Average Score -106.57\n",
      "Episode 232\n",
      "loss: 0.002070980699889938\n",
      "Average Score -106.59\n",
      "Episode 233\n",
      "loss: 1.5568199920153205\n",
      "Average Score -106.68\n",
      "Episode 234\n",
      "loss: 0.003048760619383456\n",
      "Average Score -106.69\n",
      "Episode 235\n",
      "loss: 0.0022371768260879147\n",
      "Average Score -106.75\n",
      "Episode 236\n",
      "loss: 0.0011018371316652829\n",
      "Average Score -106.96\n",
      "Episode 237\n",
      "loss: 0.00241681319027453\n",
      "Average Score -107.10\n",
      "Episode 238\n",
      "loss: 1.5566384925664196\n",
      "Average Score -107.14\n",
      "Episode 239\n",
      "loss: 1.551600142951303\n",
      "Average Score -107.19\n",
      "Episode 240\n",
      "loss: 0.00295183270165133\n",
      "Average Score -107.03\n",
      "Episode 241\n",
      "loss: 1.5562106208101258\n",
      "Average Score -107.07\n",
      "Episode 242\n",
      "loss: 0.005237583592647162\n",
      "Average Score -107.11\n",
      "Episode 243\n",
      "loss: 0.003847025701302931\n",
      "Average Score -106.89\n",
      "Episode 244\n",
      "loss: 0.0011120478681562412\n",
      "Average Score -106.61\n",
      "Episode 245\n",
      "loss: 0.0032820516834947706\n",
      "Average Score -106.63\n",
      "Episode 246\n",
      "loss: 0.003153020437048503\n",
      "Average Score -106.62\n",
      "Episode 247\n",
      "loss: 1.5548393033483396\n",
      "Average Score -106.40\n",
      "Episode 248\n",
      "loss: 1.5536545941045716\n",
      "Average Score -106.22\n",
      "Episode 249\n",
      "loss: 0.002506258157674577\n",
      "Average Score -105.97\n",
      "Episode 250\n",
      "loss: 0.0036278776106897213\n",
      "Average Score -105.74\n",
      "Episode 251\n",
      "loss: 1.5570147342879095\n",
      "Average Score -105.53\n",
      "Episode 252\n",
      "loss: 1.5517002275488032\n",
      "Average Score -105.36\n",
      "Episode 253\n",
      "loss: 0.0024461392940481\n",
      "Average Score -105.19\n",
      "Episode 254\n",
      "loss: 1.5506488955543438\n",
      "Average Score -105.14\n",
      "Episode 255\n",
      "loss: 0.0019117738611386277\n",
      "Average Score -104.92\n",
      "Episode 256\n",
      "loss: 0.004796238420634944\n",
      "Average Score -104.74\n",
      "Episode 257\n",
      "loss: 0.0032842629732301736\n",
      "Average Score -104.50\n",
      "Episode 258\n",
      "loss: 0.0024279382369868496\n",
      "Average Score -104.52\n",
      "Episode 259\n",
      "loss: 0.0028516342678969447\n",
      "Average Score -104.52\n",
      "Episode 260\n",
      "loss: 0.003979172986623622\n",
      "Average Score -104.29\n",
      "Episode 261\n",
      "loss: 0.0023126762507805617\n",
      "Average Score -104.28\n",
      "Episode 262\n",
      "loss: 1.5551990994245533\n",
      "Average Score -104.29\n",
      "Episode 263\n",
      "loss: 0.0017762584833843553\n",
      "Average Score -104.28\n",
      "Episode 264\n",
      "loss: 1.555187973998026\n",
      "Average Score -104.30\n",
      "Episode 265\n",
      "loss: 0.0023269750505799987\n",
      "Average Score -104.28\n",
      "Episode 266\n",
      "loss: 0.0036428358805891616\n",
      "Average Score -104.30\n",
      "Episode 267\n",
      "loss: 0.002415969435685615\n",
      "Average Score -104.32\n",
      "Episode 268\n",
      "loss: 4.661448744616374\n",
      "Average Score -104.33\n",
      "Episode 269\n",
      "loss: 0.0021772254752635174\n",
      "Average Score -104.35\n",
      "Episode 270\n",
      "loss: 0.0018233849865250612\n",
      "Average Score -104.50\n",
      "Episode 271\n",
      "loss: 0.002396215057869469\n",
      "Average Score -104.53\n",
      "Episode 272\n",
      "loss: 0.0022190081414931303\n",
      "Average Score -104.60\n",
      "Episode 273\n",
      "loss: 1.5509288139247863\n",
      "Average Score -104.61\n",
      "Episode 274\n",
      "loss: 0.002459090462298299\n",
      "Average Score -104.87\n",
      "Episode 275\n",
      "loss: 0.0017995390581858224\n",
      "Average Score -104.87\n",
      "Episode 276\n",
      "loss: 0.0021796181362034135\n",
      "Average Score -104.86\n",
      "Episode 277\n",
      "loss: 1.5561500344039465\n",
      "Average Score -104.82\n",
      "Episode 278\n",
      "loss: 0.0024869311172150727\n",
      "Average Score -105.10\n",
      "Episode 279\n",
      "loss: 0.002749095727668247\n",
      "Average Score -105.09\n",
      "Episode 280\n",
      "loss: 0.0024900317160897944\n",
      "Average Score -105.25\n",
      "Episode 281\n",
      "loss: 1.555901944962502\n",
      "Average Score -105.41\n",
      "Episode 282\n",
      "loss: 1.5546978430790104\n",
      "Average Score -105.45\n",
      "Episode 283\n",
      "loss: 0.004534896928821614\n",
      "Average Score -105.44\n",
      "Episode 284\n",
      "loss: 0.002771412751313267\n",
      "Average Score -105.38\n",
      "Episode 285\n",
      "loss: 1.5551955274975282\n",
      "Average Score -105.14\n",
      "Episode 286\n",
      "loss: 1.554324045706639\n",
      "Average Score -105.17\n",
      "Episode 287\n",
      "loss: 1.5553237206029398\n",
      "Average Score -105.20\n",
      "Episode 288\n",
      "loss: 0.002191135658912885\n",
      "Average Score -104.94\n",
      "Episode 289\n",
      "loss: 0.0039674185001815935\n",
      "Average Score -104.97\n",
      "Episode 290\n",
      "loss: 1.554587119912593\n",
      "Average Score -104.67\n",
      "Episode 291\n",
      "loss: 0.0024669767794087303\n",
      "Average Score -104.40\n",
      "Episode 292\n",
      "loss: 0.002443376883204567\n",
      "Average Score -104.08\n",
      "Episode 293\n",
      "loss: 3.103050327227842\n",
      "Average Score -104.11\n",
      "Episode 294\n",
      "loss: 0.0026469523769357185\n",
      "Average Score -104.15\n",
      "Episode 295\n",
      "loss: 0.0026420421668318468\n",
      "Average Score -103.89\n",
      "Episode 296\n",
      "loss: 0.00532320405742161\n",
      "Average Score -103.87\n",
      "Episode 297\n",
      "loss: 0.0035929951751086067\n",
      "Average Score -103.90\n",
      "Episode 298\n",
      "loss: 0.003363023250584915\n",
      "Average Score -103.90\n",
      "Episode 299\n",
      "loss: 1.553799125802363\n",
      "Average Score -103.94\n",
      "Episode 300\n",
      "loss: 0.0016700744058101998\n",
      "Average Score -103.94\n",
      "Episode 301\n",
      "loss: 0.0015264430244150717\n",
      "Average Score -103.95\n",
      "Episode 302\n",
      "loss: 0.0035855695934035375\n",
      "Average Score -103.94\n",
      "Episode 303\n",
      "loss: 0.005166061979063061\n",
      "Average Score -103.96\n",
      "Episode 304\n",
      "loss: 0.0030908387152336165\n",
      "Average Score -103.95\n",
      "Episode 305\n",
      "loss: 0.0029018185122715503\n",
      "Average Score -103.99\n",
      "Episode 306\n",
      "loss: 3.1018985907397627\n",
      "Average Score -104.17\n",
      "Episode 307\n",
      "loss: 3.104732429066634\n",
      "Average Score -104.16\n",
      "Episode 308\n",
      "loss: 0.002455261445458337\n",
      "Average Score -104.16\n",
      "Episode 309\n",
      "loss: 0.002714109628460628\n",
      "Average Score -104.35\n",
      "Episode 310\n",
      "loss: 0.0024688937386352174\n",
      "Average Score -104.50\n",
      "Episode 311\n",
      "loss: 3.1001754492995683\n",
      "Average Score -104.59\n",
      "Episode 312\n",
      "loss: 0.0016162722887295613\n",
      "Average Score -104.60\n",
      "Episode 313\n",
      "loss: 1.5553681253331222\n",
      "Average Score -104.58\n",
      "Episode 314\n",
      "loss: 0.0037648159405163717\n",
      "Average Score -104.64\n",
      "Episode 315\n",
      "loss: 0.00224575191820201\n",
      "Average Score -104.88\n",
      "Episode 316\n",
      "loss: 1.5560484261117369\n",
      "Average Score -104.93\n",
      "Episode 317\n",
      "loss: 1.5547563747805544\n",
      "Average Score -105.08\n",
      "Episode 318\n",
      "loss: 0.003053670766763492\n",
      "Average Score -105.09\n",
      "Episode 319\n",
      "loss: 1.5543948105377914\n",
      "Average Score -105.25\n",
      "Episode 320\n",
      "loss: 3.103680056701922\n",
      "Average Score -105.28\n",
      "Episode 321\n",
      "loss: 0.00387242090602988\n",
      "Average Score -105.29\n",
      "Episode 322\n",
      "loss: 0.0034218416889970882\n",
      "Average Score -105.31\n",
      "Episode 323\n",
      "loss: 1.5510223062787125\n",
      "Average Score -105.11\n",
      "Episode 324\n",
      "loss: 0.002717833865275682\n",
      "Average Score -105.10\n",
      "Episode 325\n",
      "loss: 0.0031216779848615146\n",
      "Average Score -105.10\n",
      "Episode 326\n",
      "loss: 0.0027324370227346365\n",
      "Average Score -105.08\n",
      "Episode 327\n",
      "loss: 1.555212519053394\n",
      "Average Score -104.92\n",
      "Episode 328\n",
      "loss: 1.5542618616113204\n",
      "Average Score -104.90\n",
      "Episode 329\n",
      "loss: 1.5542827769747627\n",
      "Average Score -104.91\n",
      "Episode 330\n",
      "loss: 0.0033233996235598304\n",
      "Average Score -104.90\n",
      "Episode 331\n",
      "loss: 0.0038992871485896653\n",
      "Average Score -104.89\n",
      "Episode 332\n",
      "loss: 0.0036641650105071113\n",
      "Average Score -104.87\n",
      "Episode 333\n",
      "loss: 0.0018116873127800328\n",
      "Average Score -104.84\n",
      "Episode 334\n",
      "loss: 0.0025505295683007708\n",
      "Average Score -104.86\n",
      "Episode 335\n",
      "loss: 0.00479230282597327\n",
      "Average Score -104.82\n",
      "Episode 336\n",
      "loss: 0.003305873471156159\n",
      "Average Score -104.63\n",
      "Episode 337\n",
      "loss: 0.0027402638696140667\n",
      "Average Score -104.53\n",
      "Episode 338\n",
      "loss: 1.553996700762419\n",
      "Average Score -104.50\n",
      "Episode 339\n",
      "loss: 0.0023428358297983313\n",
      "Average Score -104.47\n",
      "Episode 340\n",
      "loss: 1.5478684092626274\n",
      "Average Score -104.46\n",
      "Episode 341\n",
      "loss: 1.5527427574777495\n",
      "Average Score -104.29\n",
      "Episode 342\n",
      "loss: 0.00486182637872039\n",
      "Average Score -104.26\n",
      "Episode 343\n",
      "loss: 0.003824960993358216\n",
      "Average Score -104.26\n",
      "Episode 344\n",
      "loss: 0.003776235449278184\n",
      "Average Score -104.27\n",
      "Episode 345\n",
      "loss: 3.0998289990522356\n",
      "Average Score -104.26\n",
      "Episode 346\n",
      "loss: 3.1049389565967647\n",
      "Average Score -104.33\n",
      "Episode 347\n",
      "loss: 0.0026775534253565135\n",
      "Average Score -104.59\n",
      "Episode 348\n",
      "loss: 0.0037206026933718495\n",
      "Average Score -104.61\n",
      "Episode 349\n",
      "loss: 0.002343892301403479\n",
      "Average Score -104.62\n",
      "Episode 350\n",
      "loss: 0.0034756493680862047\n",
      "Average Score -104.82\n",
      "Episode 351\n",
      "loss: 0.002245122686402629\n",
      "Average Score -104.90\n",
      "Episode 352\n",
      "loss: 0.0036564715587167266\n",
      "Average Score -104.89\n",
      "Episode 353\n",
      "loss: 0.002261195054027279\n",
      "Average Score -104.90\n",
      "Episode 354\n",
      "loss: 0.004732873417298236\n",
      "Average Score -104.91\n",
      "Episode 355\n",
      "loss: 0.002714357126468617\n",
      "Average Score -104.94\n",
      "Episode 356\n",
      "loss: 0.001961686853317586\n",
      "Average Score -104.92\n",
      "Episode 357\n",
      "loss: 0.002602307551713293\n",
      "Average Score -104.95\n",
      "Episode 358\n",
      "loss: 0.0022276543461870993\n",
      "Average Score -104.96\n",
      "Episode 359\n",
      "loss: 0.003711543273504878\n",
      "Average Score -104.97\n",
      "Episode 360\n",
      "loss: 1.553460020039719\n",
      "Average Score -104.99\n",
      "Episode 361\n",
      "loss: 0.0025941955774612755\n",
      "Average Score -104.99\n",
      "Episode 362\n",
      "loss: 3.1002127410825704\n",
      "Average Score -104.99\n",
      "Episode 363\n",
      "loss: 0.0027193905954704077\n",
      "Average Score -104.99\n",
      "Episode 364\n",
      "loss: 0.0029482826146802837\n",
      "Average Score -104.98\n",
      "Episode 365\n",
      "loss: 1.5531196927521809\n",
      "Average Score -105.00\n",
      "Episode 366\n",
      "loss: 1.5528022639026382\n",
      "Average Score -104.99\n",
      "Episode 367\n",
      "loss: 0.0034262966278148345\n",
      "Average Score -104.96\n",
      "Episode 368\n",
      "loss: 0.00266701689507828\n",
      "Average Score -104.97\n",
      "Episode 369\n",
      "loss: 0.002650197949849868\n",
      "Average Score -104.98\n",
      "Episode 370\n",
      "loss: 0.0022940147900729394\n",
      "Average Score -104.84\n",
      "Episode 371\n",
      "loss: 0.001961920187804477\n",
      "Average Score -104.83\n",
      "Episode 372\n",
      "loss: 1.5552795594586344\n",
      "Average Score -104.61\n",
      "Episode 373\n",
      "loss: 1.5511845042132497\n",
      "Average Score -104.58\n",
      "Episode 374\n",
      "loss: 1.5545563814181802\n",
      "Average Score -104.32\n",
      "Episode 375\n",
      "loss: 0.0035790744071653446\n",
      "Average Score -104.29\n",
      "Episode 376\n",
      "loss: 0.004120933380630293\n",
      "Average Score -104.26\n",
      "Episode 377\n",
      "loss: 1.554993603189537\n",
      "Average Score -104.28\n",
      "Episode 378\n",
      "loss: 0.0016368913164684785\n",
      "Average Score -103.99\n",
      "Episode 379\n",
      "loss: 1.5552787065488765\n",
      "Average Score -103.97\n",
      "Episode 380\n",
      "loss: 0.0032984811417878495\n",
      "Average Score -103.80\n",
      "Episode 381\n",
      "loss: 0.0017945803901071983\n",
      "Average Score -103.59\n",
      "Episode 382\n",
      "loss: 1.5528240285587567\n",
      "Average Score -103.56\n",
      "Episode 383\n",
      "loss: 1.5548020583594186\n",
      "Average Score -103.56\n",
      "Episode 384\n",
      "loss: 0.0016949782906879813\n",
      "Average Score -103.60\n",
      "Episode 385\n",
      "loss: 0.0031513377157804693\n",
      "Average Score -103.61\n",
      "Episode 386\n",
      "loss: 1.556065387813466\n",
      "Average Score -103.63\n",
      "Episode 387\n",
      "loss: 1.5522446919129587\n",
      "Average Score -103.64\n",
      "Episode 388\n",
      "loss: 0.002074382085269246\n",
      "Average Score -103.64\n",
      "Episode 389\n",
      "loss: 0.002473192832813988\n",
      "Average Score -103.64\n",
      "Episode 390\n",
      "loss: 1.552384914891903\n",
      "Average Score -103.63\n",
      "Episode 391\n",
      "loss: 0.002995143100073581\n",
      "Average Score -103.83\n",
      "Episode 392\n",
      "loss: 1.5520139104443267\n",
      "Average Score -103.84\n",
      "Episode 393\n",
      "loss: 1.5498193519383812\n",
      "Average Score -104.00\n",
      "Episode 394\n",
      "loss: 0.0035711068663545663\n",
      "Average Score -104.21\n",
      "Episode 395\n",
      "loss: 0.0016487336756153552\n",
      "Average Score -104.23\n",
      "Episode 396\n",
      "loss: 0.0020532052911515957\n",
      "Average Score -104.25\n",
      "Episode 397\n",
      "loss: 0.0028455345644558686\n",
      "Average Score -104.42\n",
      "Episode 398\n",
      "loss: 1.551270681387882\n",
      "Average Score -104.63\n",
      "Episode 399\n",
      "loss: 1.5520457284204792\n",
      "Average Score -104.59\n",
      "Episode 400\n",
      "loss: 0.001258318142313486\n",
      "Average Score -104.75\n",
      "Episode 401\n",
      "loss: 1.5533787434528417\n",
      "Average Score -105.09\n",
      "Episode 402\n",
      "loss: 0.0025815767184893925\n",
      "Average Score -105.13\n",
      "Episode 403\n",
      "loss: 1.55137279437913\n",
      "Average Score -105.11\n",
      "Episode 404\n",
      "loss: 1.5515827251348382\n",
      "Average Score -105.12\n",
      "Episode 405\n",
      "loss: 0.001874806615255145\n",
      "Average Score -105.08\n",
      "Episode 406\n",
      "loss: 0.0015695973174884097\n",
      "Average Score -104.90\n",
      "Episode 407\n",
      "loss: 1.5510827872713622\n",
      "Average Score -104.88\n",
      "Episode 408\n",
      "loss: 1.5565059612583019\n",
      "Average Score -104.89\n",
      "Episode 409\n",
      "loss: 0.001987702281371684\n",
      "Average Score -104.71\n",
      "Episode 410\n",
      "loss: 0.002515036100546472\n",
      "Average Score -104.57\n",
      "Episode 411\n",
      "loss: 1.5525086691697314\n",
      "Average Score -104.55\n",
      "Episode 412\n",
      "loss: 0.0026345600483852743\n",
      "Average Score -104.63\n",
      "Episode 413\n",
      "loss: 0.0009924297724893624\n",
      "Average Score -104.70\n",
      "Episode 414\n",
      "loss: 0.0031459756101190994\n",
      "Average Score -104.65\n",
      "Episode 415\n",
      "loss: 0.0025825545254406354\n",
      "Average Score -104.42\n",
      "Episode 416\n",
      "loss: 0.0030154338020428545\n",
      "Average Score -104.41\n",
      "Episode 417\n",
      "loss: 0.0022799023525906285\n",
      "Average Score -104.26\n",
      "Episode 418\n",
      "loss: 0.0024651066333869864\n",
      "Average Score -104.28\n",
      "Episode 419\n",
      "loss: 1.5546916905425248\n",
      "Average Score -104.14\n",
      "Episode 420\n",
      "loss: 0.0014118088905537974\n",
      "Average Score -104.13\n",
      "Episode 421\n",
      "loss: 0.00319580575041775\n",
      "Average Score -104.12\n",
      "Episode 422\n",
      "loss: 0.001951613013806976\n",
      "Average Score -104.11\n",
      "Episode 423\n",
      "loss: 0.002502759385033099\n",
      "Average Score -104.09\n",
      "Episode 424\n",
      "loss: 0.0025666265504321537\n",
      "Average Score -104.12\n",
      "Episode 425\n",
      "loss: 0.0025923684836802663\n",
      "Average Score -104.13\n",
      "Episode 426\n",
      "loss: 0.002080253904267514\n",
      "Average Score -104.13\n",
      "Episode 427\n",
      "loss: 0.002268132773596068\n",
      "Average Score -104.27\n",
      "Episode 428\n",
      "loss: 0.0017639224632048519\n",
      "Average Score -104.35\n",
      "Episode 429\n",
      "loss: 0.0025612406616019653\n",
      "Average Score -104.57\n",
      "Episode 430\n",
      "loss: 0.00175784829595891\n",
      "Average Score -104.67\n",
      "Episode 431\n",
      "loss: 1.553507284030533\n",
      "Average Score -104.88\n",
      "Episode 432\n",
      "loss: 1.5496479786103379\n",
      "Average Score -105.08\n",
      "Episode 433\n",
      "loss: 3.0975534245158602\n",
      "Average Score -105.08\n",
      "Episode 434\n",
      "loss: 1.5524771737058012\n",
      "Average Score -105.14\n",
      "Episode 435\n",
      "loss: 0.0023885367076862457\n",
      "Average Score -105.19\n",
      "Episode 436\n",
      "loss: 1.5514993054692068\n",
      "Average Score -105.17\n",
      "Episode 437\n",
      "loss: 0.0030462742164664635\n",
      "Average Score -105.17\n",
      "Episode 438\n",
      "loss: 3.1021002268860505\n",
      "Average Score -105.38\n",
      "Episode 439\n",
      "loss: 1.5519444024505393\n",
      "Average Score -105.57\n",
      "Episode 440\n",
      "loss: 1.5515799954151483\n",
      "Average Score -105.56\n",
      "Episode 441\n",
      "loss: 0.002896530361482821\n",
      "Average Score -105.79\n",
      "Episode 442\n",
      "loss: 0.0026687236511572497\n",
      "Average Score -105.77\n",
      "Episode 443\n",
      "loss: 0.004023057527058132\n",
      "Average Score -105.80\n",
      "Episode 444\n",
      "loss: 1.5531737177194773\n",
      "Average Score -105.81\n",
      "Episode 445\n",
      "loss: 0.0038460857382838515\n",
      "Average Score -105.84\n",
      "Episode 446\n",
      "loss: 0.0015211299441523223\n",
      "Average Score -105.80\n",
      "Episode 447\n",
      "loss: 1.5531433446164564\n",
      "Average Score -105.55\n",
      "Episode 448\n",
      "loss: 0.0041805355225261204\n",
      "Average Score -105.54\n",
      "Episode 449\n",
      "loss: 1.5499636698555854\n",
      "Average Score -105.55\n",
      "Episode 450\n",
      "loss: 0.0041731533058062716\n",
      "Average Score -105.38\n",
      "Episode 451\n",
      "loss: 0.0038307056475493737\n",
      "Average Score -105.37\n",
      "Episode 452\n",
      "loss: 0.0022734000438649603\n",
      "Average Score -105.44\n",
      "Episode 453\n",
      "loss: 0.0028065694682504567\n",
      "Average Score -105.54\n",
      "Episode 454\n",
      "loss: 1.5525203329429598\n",
      "Average Score -105.58\n",
      "Episode 455\n",
      "loss: 0.0033243752978915683\n",
      "Average Score -105.59\n",
      "Episode 456\n",
      "loss: 1.5532311605404232\n",
      "Average Score -105.61\n",
      "Episode 457\n",
      "loss: 0.0021409536229431273\n",
      "Average Score -105.69\n",
      "Episode 458\n",
      "loss: 0.0038811295832577586\n",
      "Average Score -105.80\n",
      "Episode 459\n",
      "loss: 1.5509333026028442\n",
      "Average Score -105.93\n",
      "Episode 460\n",
      "loss: 1.5531474289750027\n",
      "Average Score -105.90\n",
      "Episode 461\n",
      "loss: 0.0027784408980298797\n",
      "Average Score -105.99\n",
      "Episode 462\n",
      "loss: 0.003593437008889657\n",
      "Average Score -105.99\n",
      "Episode 463\n",
      "loss: 0.002672548829516012\n",
      "Average Score -106.04\n",
      "Episode 464\n",
      "loss: 3.0980144254646165\n",
      "Average Score -106.12\n",
      "Episode 465\n",
      "loss: 0.0017938195791759903\n",
      "Average Score -106.17\n",
      "Episode 466\n",
      "loss: 1.552442503192672\n",
      "Average Score -106.21\n",
      "Episode 467\n",
      "loss: 1.55508301680935\n",
      "Average Score -106.25\n",
      "Episode 468\n",
      "loss: 0.002616151044734954\n",
      "Average Score -106.32\n",
      "Episode 469\n",
      "loss: 0.001909988037508709\n",
      "Average Score -106.35\n",
      "Episode 470\n",
      "loss: 1.554755216686046\n",
      "Average Score -106.40\n",
      "Episode 471\n",
      "loss: 0.002804038584531738\n",
      "Average Score -106.46\n",
      "Episode 472\n",
      "loss: 1.5515353893491781\n",
      "Average Score -106.52\n",
      "Episode 473\n",
      "loss: 3.1012440876791474\n",
      "Average Score -106.58\n",
      "Episode 474\n",
      "loss: 1.5505337477754744\n",
      "Average Score -106.68\n",
      "Episode 475\n",
      "loss: 1.5532869737216997\n",
      "Average Score -106.77\n",
      "Episode 476\n",
      "loss: 3.103072482488099\n",
      "Average Score -106.89\n",
      "Episode 477\n",
      "loss: 0.0024223679714999538\n",
      "Average Score -106.97\n",
      "Episode 478\n",
      "loss: 0.004223920947795411\n",
      "Average Score -107.05\n",
      "Episode 479\n",
      "loss: 1.5508606070181472\n",
      "Average Score -107.13\n",
      "Episode 480\n",
      "loss: 1.5518208741571515\n",
      "Average Score -107.18\n",
      "Episode 481\n",
      "loss: 0.0035814686578544497\n",
      "Average Score -107.26\n",
      "Episode 482\n",
      "loss: 1.551229597797307\n",
      "Average Score -107.33\n",
      "Episode 483\n",
      "loss: 1.5550571963404034\n",
      "Average Score -107.35\n",
      "Episode 484\n",
      "loss: 1.5515309075587052\n",
      "Average Score -107.42\n",
      "Episode 485\n",
      "loss: 1.5520579760716273\n",
      "Average Score -107.50\n",
      "Episode 486\n",
      "loss: 0.005379509521484465\n",
      "Average Score -107.50\n",
      "Episode 487\n",
      "loss: 1.5522587285318297\n",
      "Average Score -107.49\n",
      "Episode 488\n",
      "loss: 0.003449875557723197\n",
      "Average Score -107.56\n",
      "Episode 489\n",
      "loss: 0.0017554595792747853\n",
      "Average Score -107.56\n",
      "Episode 490\n",
      "loss: 0.004449960679394821\n",
      "Average Score -107.64\n",
      "Episode 491\n",
      "loss: 3.1005965431575757\n",
      "Average Score -107.43\n",
      "Episode 492\n",
      "loss: 0.0030033948027371193\n",
      "Average Score -107.40\n",
      "Episode 493\n",
      "loss: 0.0021672651587648865\n",
      "Average Score -107.22\n",
      "Episode 494\n",
      "loss: 0.005079102089871033\n",
      "Average Score -107.01\n",
      "Episode 495\n",
      "loss: 0.002195338254197743\n",
      "Average Score -107.01\n",
      "Episode 496\n",
      "loss: 3.1005652490153537\n",
      "Average Score -107.05\n",
      "Episode 497\n",
      "loss: 0.0025398867254760185\n",
      "Average Score -106.86\n",
      "Episode 498\n",
      "loss: 1.5512203473415083\n",
      "Average Score -106.63\n",
      "Episode 499\n",
      "loss: 0.004009204807882\n",
      "Average Score -106.65\n",
      "Episode 500\n",
      "loss: 0.002197859492059058\n",
      "Average Score -106.52\n",
      "Episode 501\n",
      "loss: 0.0032194784031029525\n",
      "Average Score -106.25\n",
      "Episode 502\n",
      "loss: 0.0032886152689010992\n",
      "Average Score -106.23\n",
      "Episode 503\n",
      "loss: 0.0032578427822462196\n",
      "Average Score -106.22\n",
      "Episode 504\n",
      "loss: 0.003043962485440801\n",
      "Average Score -106.21\n",
      "Episode 505\n",
      "loss: 1.5508374390251563\n",
      "Average Score -106.19\n",
      "Episode 506\n",
      "loss: 0.0030994090270264544\n",
      "Average Score -106.19\n",
      "Episode 507\n",
      "loss: 1.5525041337994925\n",
      "Average Score -106.19\n",
      "Episode 508\n",
      "loss: 0.0019131439363714135\n",
      "Average Score -106.22\n",
      "Episode 509\n",
      "loss: 0.0028404656303727066\n",
      "Average Score -106.43\n",
      "Episode 510\n",
      "loss: 1.552960264583354\n",
      "Average Score -106.44\n",
      "Episode 511\n",
      "loss: 1.5515345313415485\n",
      "Average Score -106.36\n",
      "Episode 512\n",
      "loss: 1.549991253308977\n",
      "Average Score -106.50\n",
      "Episode 513\n",
      "loss: 0.002412687515820589\n",
      "Average Score -106.66\n",
      "Episode 514\n",
      "loss: 0.00359602641758408\n",
      "Average Score -106.86\n",
      "Episode 515\n",
      "loss: 0.0023702112972120827\n",
      "Average Score -107.08\n",
      "Episode 516\n",
      "loss: 1.5518960272614362\n",
      "Average Score -107.29\n",
      "Episode 517\n",
      "loss: 0.004753376963255122\n",
      "Average Score -107.29\n",
      "Episode 518\n",
      "loss: 3.098775131564536\n",
      "Average Score -107.34\n",
      "Episode 519\n",
      "loss: 1.552934002520712\n",
      "Average Score -107.57\n",
      "Episode 520\n",
      "loss: 0.003092006036171943\n",
      "Average Score -107.55\n",
      "Episode 521\n",
      "loss: 1.5514402665241998\n",
      "Average Score -107.78\n",
      "Episode 522\n",
      "loss: 1.551261487687455\n",
      "Average Score -107.99\n",
      "Episode 523\n",
      "loss: 1.5508165875409985\n",
      "Average Score -107.96\n",
      "Episode 524\n",
      "loss: 0.002442508088797603\n",
      "Average Score -107.95\n",
      "Episode 525\n",
      "loss: 0.0030395417890511788\n",
      "Average Score -107.94\n",
      "Episode 526\n",
      "loss: 0.0030941012359934216\n",
      "Average Score -107.91\n",
      "Episode 527\n",
      "loss: 1.5549373074404875\n",
      "Average Score -107.76\n",
      "Episode 528\n",
      "loss: 0.002482783936720182\n",
      "Average Score -107.70\n",
      "Episode 529\n",
      "loss: 0.0030576929202929975\n",
      "Average Score -107.45\n",
      "Episode 530\n",
      "loss: 1.552280431884993\n",
      "Average Score -107.50\n",
      "Episode 531\n",
      "loss: 1.5515330579811433\n",
      "Average Score -107.29\n",
      "Episode 532\n",
      "loss: 0.004327777601361285\n",
      "Average Score -107.06\n",
      "Episode 533\n",
      "loss: 1.555284278694083\n",
      "Average Score -107.06\n",
      "Episode 534\n",
      "loss: 1.553534162059273\n",
      "Average Score -107.03\n",
      "Episode 535\n",
      "loss: 1.55178845748448\n",
      "Average Score -107.00\n",
      "Episode 536\n",
      "loss: 0.004027584996704608\n",
      "Average Score -107.02\n",
      "Episode 537\n",
      "loss: 0.004111503362740413\n",
      "Average Score -107.03\n",
      "Episode 538\n",
      "loss: 1.5528659896088097\n",
      "Average Score -106.84\n",
      "Episode 539\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: clip() received an invalid combination of arguments - got (int, int, out=NoneType), but expected one of:\n * (Tensor min, Tensor max)\n * (Number min, Number max)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, losses\n\u001b[0;32m---> 49\u001b[0m scores, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m plot_returns(scores)\n\u001b[1;32m     51\u001b[0m plot_returns(losses)\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_episodes, max_t)\u001b[0m\n\u001b[1;32m     24\u001b[0m index \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m     25\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdisct2cont(index)\n\u001b[0;32m---> 27\u001b[0m next_observation, reward, done, truncated, _  \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_observation, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward],device \u001b[38;5;241m=\u001b[39m device)\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/gymnasium/envs/box2d/bipedal_walker.py:597\u001b[0m, in \u001b[0;36mBipedalWalker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_shaping \u001b[38;5;241m=\u001b[39m shaping\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m action:\n\u001b[0;32m--> 597\u001b[0m     reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00035\u001b[39m \u001b[38;5;241m*\u001b[39m MOTORS_TORQUE \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;66;03m# normalized to about -50.0 using heuristic, more optimal agent should spend less\u001b[39;00m\n\u001b[1;32m    600\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2180\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2178\u001b[0m \n\u001b[1;32m   2179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multi_pred/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_space = env.observation_space, action_space = env.action_space, disc_class = 20, seed = 0)\n",
    "\n",
    "def train(n_episodes = 2000, max_t = 1600):\n",
    "    \"\"\"\n",
    "    Training DQN\n",
    "    \n",
    "    @param n_episodes(int): Number of training episodes\n",
    "    @param max_t(int):      Maximum number of timesteps per episode\n",
    "    eps_start(float):       Starting value of epsilon for epsilon-greedy action selection\n",
    "    eps_end(float):         Minimum value of epsilon\n",
    "    eps_dec(float):         Multiplicative factor for decrementing epsilon\n",
    "    \"\"\"\n",
    "    # For plotting\n",
    "    scores = [ ]\n",
    "    losses = [ ]\n",
    "    scores_window = deque(maxlen=100) \n",
    "    for episode in range(1,n_episodes+1):\n",
    "        print('\\rEpisode {}'.format(episode), end=\"\\n\")\n",
    "        observation, info = env.reset()\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        score = 0\n",
    "        done = False\n",
    "        for t in range(max_t):\n",
    "            index = agent.choose_action(state)\n",
    "            action = agent.disct2cont(index)\n",
    "            \n",
    "            next_observation, reward, done, truncated, _  = env.step(action)\n",
    "            next_state = torch.tensor(next_observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            reward = torch.tensor([reward],device = device)\n",
    "            score += reward[0]\n",
    "            agent.add_memory(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        loss = agent.learn()\n",
    "        # For plotting\n",
    "        print(\"loss:\",loss)\n",
    "        losses.append(loss)\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rAverage Score {:.2f}'.format(np.mean(scores_window)), end=\"\\n\")\n",
    "        if score > 300:\n",
    "            print(f\"Environment solved! t:{t}, episode:{episode}\")\n",
    "            break\n",
    "    return scores, losses\n",
    "\n",
    "scores, losses = train()\n",
    "plot_returns(scores)\n",
    "plot_returns(losses)\n",
    "#plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(losses)),losses)\n",
    "plt.ylabel('Losses')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106b548-14be-4c09-bf55-73b146ee51c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
